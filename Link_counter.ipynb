{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Link_counter.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bitdough/learning/blob/master/Link_counter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwMKsgnWRLjj",
        "colab_type": "text"
      },
      "source": [
        "# Twitter External Link Analysis Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqVb_9LtRTWN",
        "colab_type": "text"
      },
      "source": [
        "This code takes a list of twitter usernames, iterates over them to find tweets where they shared links, and then sums up the base URLs of everyone's links combined and turns it into a bar graph. Please check code documentation for usage guidance. The code does take a bit to run depending on your tweet limit and how many accounts you pull. Note: this code writes some CSV's to the hosted drive file to store the data. Running this code repeatedly without refreshing each time before hand may add extra links to the end visualization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vns7DUleRnJW",
        "colab_type": "text"
      },
      "source": [
        "##Instructions to run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB3GP1CgRpOQ",
        "colab_type": "text"
      },
      "source": [
        "1.   Click Open in playground mode in top-left corner\n",
        "2.   Write your list of usernames to analyze between the brackets below using the format described.\n",
        "3.   Click Runtime in menu at top, click run all.\n",
        "4.   Visualization should be at the bottom\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIa_DN5eSNgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#put accounts in between the brackets, comma seperated, without the @sign. ie [\"jack\", \"realDonaldtrump\", \"Blacksocialists\"]\n",
        "#don't do a ton of accounts or it will take forever\n",
        "sourceAccounts= [\"PUT YOUR ACCOUNTS HERE\" , \"DIRECTIONS ABOVE\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3GpFlvN-_wu",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhOrjh1MRDu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install twint\n",
        "#this prevents async problems/ runtime errors\n",
        "#https://markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/\n",
        "!pip install nest_asyncio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV9SAEMkRGY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This code takes a list of twitter usernames, iterates over them to find tweets where they shared links,\n",
        "#and then sums up the base URLs of everyones links combined and turns it into a matplotlib graph.\n",
        "#I put a bunch of code documentation in and it really will help you use this.\n",
        "#the code does take a bit to run depending on your tweet limit and how many accounts you pull\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import twint #you may need to install this first if you haven't!\n",
        "\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import csv \n",
        "import os\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "\n",
        "with open('all_urls.csv', 'wb') as f:\n",
        "  pass\n",
        "\n",
        "allURLs = []\n",
        "for username in sourceAccounts:\n",
        "    c = twint.Config()\n",
        "    print(\"pulling tweets for \" + str(username) + \"...\")\n",
        "    print(\"ignore twint errors\")\n",
        "    c.Username = username\n",
        "    c.Hide_output = True #makes the command line less noisy\n",
        "    c.Limit = 500 #maximum number of tweets to pull per account\n",
        "    c.Store_object = True\n",
        "    #only selects tweets that have links\n",
        "    c.Links = \"include\"\n",
        "\n",
        "\n",
        "    baseURLs = []\n",
        "    twint.run.Search(c)\n",
        "    tweets = twint.output.tweets_list\n",
        "    for tweet in tweets:\n",
        "        #urls is a class in the twint tweet objects to see all classes: dir(tweet)\n",
        "        for URL in tweet.urls:\n",
        "\n",
        "            parsed_uri = urlparse(URL)\n",
        "            baseURL = str('{uri.netloc}'.format(uri=parsed_uri)) #gets the base URL\n",
        "            if baseURL[:7] == 'twitter': #ignores RTs as links\n",
        "                pass\n",
        "            elif baseURL[:4] == \"www.\": #strips www for a e s t h e t i c\n",
        "                baseURLs.append([username, baseURL[4:]])\n",
        "            else:\n",
        "                baseURLs.append([username, baseURL])\n",
        "\n",
        "    # all_urls += baseURLs\n",
        "    # I added this in case it gets slow in pulling the list so you can stop at any point and then just\n",
        "    # edit your sourceAccounts list to get rid of the one's you've already done.\n",
        "    with open('all_urls.csv','a', newline='') as f:\n",
        "        for baseURL in baseURLs:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(baseURL)\n",
        "\n",
        "\n",
        "\n",
        "all_urls = pd.read_csv('all_urls.csv', names = ['username','URL'])\n",
        "# all_urls = all_urls.DataFrame(names = ['username','URL'])\n",
        "\n",
        "print(\"total tweets pulled: \" + str(len(all_urls)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkrbmBsOTQii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = ['Base URL', 'Frequency']\n",
        "countedURLs = all_urls['URL'].value_counts()\n",
        "countedURLs.to_csv('countedURLs.csv')\n",
        "\n",
        "top_urls = countedURLs.iloc[:10]\n",
        "top_urls = top_urls[::-1] #makes it descending\n",
        "\n",
        "y_pos = np.arange(len(top_urls))\n",
        "performance = top_urls\n",
        "print(performance)\n",
        "baseURLs =  top_urls.index\n",
        "print(baseURLs)\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, baseURLs)\n",
        "plt.xlabel('Frequency of Links')\n",
        "plt.title('Most Frequent External Links of all Handles Tested')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}